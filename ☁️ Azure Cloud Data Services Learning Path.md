<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# â˜ï¸ Azure Cloud Data Services Learning Path

Let me break down this comprehensive Azure data engineering journey with detailed explanations and practical examples!

***

## ğŸ“… Days 8-9: Azure Data Factory (ADF)

### ğŸ—ï¸ Azure Data Factory Architecture

**Azure Data Factory** is Microsoft's cloud-based ETL (Extract, Transform, Load) and data integration service. Think of it as the *orchestration conductor* ğŸ¼ of your data pipelines.

#### Core Components:

**1ï¸âƒ£ Pipelines** ğŸ”„
Pipelines are logical groupings of activities that perform a data workflow task. They're like a recipe ğŸ“ - a series of steps to transform ingredients (data) into a final dish (insights).

```python
# Example: Creating a simple pipeline using Azure SDK
from azure.identity import DefaultAzureCredential
from azure.mgmt.datafactory import DataFactoryManagementClient
from azure.mgmt.datafactory.models import *

# Pipeline definition
pipeline = {
    "activities": [
        {
            "name": "CopyFromBlobToSQL",
            "type": "Copy",
            "inputs": [{"referenceName": "SourceDataset"}],
            "outputs": [{"referenceName": "SinkDataset"}]
        }
    ]
}
```

**2ï¸âƒ£ Activities** âš™ï¸
Activities are individual processing steps within a pipeline. Types include:

- **Data Movement Activities**: Copy data between stores ğŸ“¦â¡ï¸ğŸ“¦
- **Data Transformation Activities**: Transform data using compute services ğŸ”§
- **Control Activities**: Manage pipeline flow (If, ForEach, Wait) ğŸ®

```json
{
  "name": "CopyActivity",
  "type": "Copy",
  "typeProperties": {
    "source": {
      "type": "BlobSource"
    },
    "sink": {
      "type": "SqlSink",
      "writeBatchSize": 10000
    }
  }
}
```

**3ï¸âƒ£ Linked Services** ğŸ”—
Linked Services are like *connection strings* - they define connection information to external resources (databases, storage accounts, APIs).

```json
{
  "name": "AzureBlobStorageLinkedService",
  "type": "AzureBlobStorage",
  "typeProperties": {
    "connectionString": "DefaultEndpointsProtocol=https;AccountName=mystorageaccount;AccountKey=<key>"
  }
}
```

**4ï¸âƒ£ Datasets** ğŸ“Š
Datasets represent data structures within data stores. They point to the data you want to use in your activities.

```json
{
  "name": "InputDataset",
  "type": "AzureBlob",
  "linkedServiceName": "AzureBlobStorageLinkedService",
  "typeProperties": {
    "folderPath": "input/files/",
    "fileName": "data.csv",
    "format": {
      "type": "TextFormat",
      "columnDelimiter": ","
    }
  }
}
```


### ğŸ–¥ï¸ Integration Runtimes (IR)

Integration Runtime is the **compute infrastructure** ğŸ’» ADF uses to provide data integration capabilities across different network environments.

**Types of Integration Runtimes:**

**Azure IR** â˜ï¸

- Fully managed, serverless compute in Azure
- Best for cloud-to-cloud data movement
- Automatically scales based on workload
- No infrastructure management needed

```python
# Azure IR is the default - no special configuration needed
# Used automatically for cloud data sources
```

**Self-hosted IR** ğŸ¢

- Installed on your on-premises machine or VM
- Required for accessing on-premises data sources
- Enables hybrid data integration scenarios
- You manage the infrastructure

```python
# Installing Self-hosted IR (PowerShell example)
# Download and install the Self-hosted IR software
# Register it with your Data Factory using authentication key

# Configuration example
{
  "name": "OnPremisesIR",
  "type": "SelfHosted",
  "description": "Self-hosted IR for on-prem SQL Server"
}
```

**Azure-SSIS IR** ğŸ“¦

- Lift-and-shift SSIS packages to Azure
- Runs SQL Server Integration Services packages in the cloud


### ğŸŒŠ Data Flows, Copy Activities \& Transformations

**Copy Activities** ğŸ“‹
The workhorse of ADF - moves data from source to destination with optional transformations.

```python
# Copy Activity with transformations
copy_activity = {
    "name": "CopyWithTransform",
    "type": "Copy",
    "source": {
        "type": "DelimitedTextSource",
        "storeSettings": {
            "type": "AzureBlobStorageReadSettings"
        }
    },
    "sink": {
        "type": "AzureSqlSink",
        "preCopyScript": "TRUNCATE TABLE dbo.TargetTable"
    },
    "translator": {
        "type": "TabularTranslator",
        "mappings": [
            {
                "source": {"name": "FirstName"},
                "sink": {"name": "first_name"}
            },
            {
                "source": {"name": "LastName"},
                "sink": {"name": "last_name"}
            }
        ]
    }
}
```

**Data Flows** ğŸ”€
Visual data transformation tool with code-free experience. Uses Spark clusters under the hood.

```python
# Mapping Data Flow example (JSON representation)
data_flow = {
    "name": "TransformCustomerData",
    "type": "MappingDataFlow",
    "typeProperties": {
        "sources": [
            {
                "name": "SourceCustomers",
                "dataset": "CustomersDataset"
            }
        ],
        "transformations": [
            {
                "name": "FilterActive",
                "type": "Filter",
                "expression": "status == 'active'"
            },
            {
                "name": "DerivedColumn",
                "type": "DerivedColumn",
                "columns": [
                    {
                        "name": "full_name",
                        "expression": "concat(first_name, ' ', last_name)"
                    }
                ]
            },
            {
                "name": "AggregateByCountry",
                "type": "Aggregate",
                "groupBy": ["country"],
                "aggregates": [
                    {
                        "name": "customer_count",
                        "expression": "count()"
                    }
                ]
            }
        ],
        "sinks": [
            {
                "name": "SinkToSQL",
                "dataset": "OutputDataset"
            }
        ]
    }
}
```


### â° ADF Triggers, Scheduling \& Monitoring

**Triggers** ğŸ¯ automate pipeline execution:

**1. Schedule Trigger** ğŸ“…
Runs pipelines on a time-based schedule (like cron jobs).

```json
{
  "name": "DailyTrigger",
  "type": "ScheduleTrigger",
  "typeProperties": {
    "recurrence": {
      "frequency": "Day",
      "interval": 1,
      "startTime": "2025-01-01T00:00:00Z",
      "timeZone": "UTC",
      "schedule": {
        "hours": [2],
        "minutes": [0]
      }
    }
  }
}
```

**2. Tumbling Window Trigger** ğŸªŸ
For processing data in periodic, non-overlapping time windows.

```json
{
  "name": "HourlyWindow",
  "type": "TumblingWindowTrigger",
  "typeProperties": {
    "frequency": "Hour",
    "interval": 1,
    "startTime": "2025-01-01T00:00:00Z",
    "maxConcurrency": 5
  }
}
```

**3. Event-based Trigger** âš¡
Fires when specific events occur (like file arrival in Blob Storage).

```json
{
  "name": "BlobEventTrigger",
  "type": "BlobEventsTrigger",
  "typeProperties": {
    "blobPathBeginsWith": "/container/input/",
    "blobPathEndsWith": ".csv",
    "events": ["Microsoft.Storage.BlobCreated"]
  }
}
```

**Monitoring** ğŸ“Š

```python
# Monitoring pipeline runs using Python SDK
from azure.mgmt.datafactory import DataFactoryManagementClient
from datetime import datetime, timedelta

# Initialize client
client = DataFactoryManagementClient(credential, subscription_id)

# Query pipeline runs from last 24 hours
filter_params = {
    "lastUpdatedAfter": datetime.now() - timedelta(days=1),
    "lastUpdatedBefore": datetime.now()
}

pipeline_runs = client.pipeline_runs.query_by_factory(
    resource_group_name="myResourceGroup",
    factory_name="myDataFactory",
    filter_parameters=filter_params
)

# Check run status
for run in pipeline_runs.value:
    print(f"Pipeline: {run.pipeline_name}")
    print(f"Status: {run.status}")
    print(f"Duration: {run.duration_in_ms}ms")
```


***

## ğŸ“… Days 10-11: Azure Databricks + ADLS Gen2

### ğŸ—„ï¸ ADLS Gen2 Storage Patterns

**Azure Data Lake Storage Gen2** combines the capabilities of Azure Blob Storage with a hierarchical file system. It's like having a **supercharged file system** ğŸš€ in the cloud!

**Key Features:**

- Hierarchical namespace (folders/directories) ğŸ“
- POSIX-compliant access control ğŸ”
- Optimized for big data analytics ğŸ“ˆ
- Cost-effective storage tiers ğŸ’°

**Storage Patterns:**

```python
# ADLS Gen2 structure example
"""
mycontainer/
â”œâ”€â”€ raw/                    # Landing zone for ingested data
â”‚   â”œâ”€â”€ source1/
â”‚   â”‚   â””â”€â”€ 2025/01/10/data.json
â”‚   â””â”€â”€ source2/
â”‚       â””â”€â”€ 2025/01/10/data.csv
â”œâ”€â”€ processed/              # Cleaned and transformed data
â”‚   â”œâ”€â”€ customers/
â”‚   â”‚   â””â”€â”€ year=2025/month=01/data.parquet
â”‚   â””â”€â”€ orders/
â”‚       â””â”€â”€ year=2025/month=01/data.parquet
â””â”€â”€ curated/               # Business-ready datasets
    â”œâ”€â”€ customer_360/
    â”‚   â””â”€â”€ customer_profile.delta
    â””â”€â”€ sales_analytics/
        â””â”€â”€ daily_sales.delta
"""
```


### ğŸ”ï¸ Mounting ADLS Gen2 in Databricks

**Service Principal Method** (Recommended for production) ğŸ”‘

```python
# Mount ADLS Gen2 using Service Principal
configs = {
    "fs.azure.account.auth.type": "OAuth",
    "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
    "fs.azure.account.oauth2.client.id": "<application-id>",
    "fs.azure.account.oauth2.client.secret": "<service-credential>",
    "fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/<directory-id>/oauth2/token"
}

# Mount the storage account
dbutils.fs.mount(
    source="abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/",
    mount_point="/mnt/datalake",
    extra_configs=configs
)

# Verify mount
display(dbutils.fs.ls("/mnt/datalake"))
```

**Access Key Method** (For development/testing) ğŸ—ï¸

```python
# Set access key configuration
spark.conf.set(
    "fs.azure.account.key.<storage-account-name>.dfs.core.windows.net",
    "<storage-account-access-key>"
)

# Access data directly
df = spark.read.parquet("abfss://<container>@<storage-account>.dfs.core.windows.net/data/")
```


### ğŸ›ï¸ Data Lake Architecture (Raw/Processed/Curated Zones)

The **Medallion Architecture** ğŸ¥‡ğŸ¥ˆğŸ¥‰ is a best practice for organizing data lakes:

**Bronze Layer (Raw)** ğŸ¥‰

- Raw, unprocessed data as ingested from sources
- Preserves source data fidelity
- Append-only, immutable

```python
# Writing to Bronze layer
raw_df = spark.read.json("source_data/")

(raw_df
    .write
    .mode("append")
    .partitionBy("ingestion_date")
    .format("parquet")
    .save("/mnt/datalake/raw/customer_data/")
)
```

**Silver Layer (Processed)** ğŸ¥ˆ

- Cleaned, validated, deduplicated data
- Applied business rules and standardization
- Optimized for analytics

```python
# Processing to Silver layer
from pyspark.sql.functions import col, current_timestamp, regexp_replace

bronze_df = spark.read.parquet("/mnt/datalake/raw/customer_data/")

# Clean and transform
silver_df = (bronze_df
    .dropDuplicates(["customer_id"])
    .filter(col("email").isNotNull())
    .withColumn("phone_clean", regexp_replace(col("phone"), "[^0-9]", ""))
    .withColumn("processed_timestamp", current_timestamp())
)

# Write to Silver
(silver_df
    .write
    .mode("overwrite")
    .format("delta")
    .partitionBy("country", "year", "month")
    .save("/mnt/datalake/processed/customers/")
)
```

**Gold Layer (Curated)** ğŸ¥‡

- Business-level aggregates and feature tables
- Ready for consumption by BI tools and ML models
- Highly denormalized and optimized

```python
# Creating Gold layer aggregates
from pyspark.sql.functions import sum, count, avg

silver_customers = spark.read.format("delta").load("/mnt/datalake/processed/customers/")
silver_orders = spark.read.format("delta").load("/mnt/datalake/processed/orders/")

# Create customer 360 view
gold_customer_360 = (silver_customers
    .join(silver_orders, "customer_id", "left")
    .groupBy("customer_id", "customer_name", "email", "country")
    .agg(
        count("order_id").alias("total_orders"),
        sum("order_amount").alias("lifetime_value"),
        avg("order_amount").alias("avg_order_value")
    )
)

# Write to Gold
(gold_customer_360
    .write
    .mode("overwrite")
    .format("delta")
    .save("/mnt/datalake/curated/customer_360/")
)
```


### ğŸ““ Databricks Notebooks \& Cluster Management

**Databricks Notebooks** ğŸ“ are interactive documents combining code, visualizations, and narrative text (like Jupyter notebooks on steroids! ğŸ’ª).

**Creating a Notebook:**

```python
# Cell 1: Setup and imports
from pyspark.sql.functions import *
from delta.tables import *

# Display current database
print(f"Current Database: {spark.catalog.currentDatabase()}")

# Cell 2: Read data
df = spark.read.format("delta").load("/mnt/datalake/processed/customers/")

# Cell 3: Transform
transformed_df = df.filter(col("status") == "active").select("customer_id", "name", "email")

# Cell 4: Visualize (using displayHTML or display)
display(transformed_df.groupBy("country").count().orderBy(desc("count")))

# Cell 5: Write results
transformed_df.write.format("delta").mode("overwrite").save("/mnt/output/active_customers/")
```

**Cluster Management** ğŸ–¥ï¸

Clusters are sets of computation resources and configurations that run your data processing workloads.

**Cluster Types:**

```python
# All-Purpose Cluster Configuration (Interactive work)
cluster_config = {
    "cluster_name": "interactive-cluster",
    "spark_version": "13.3.x-scala2.12",
    "node_type_id": "Standard_DS3_v2",
    "num_workers": 2,  # Or use autoscaling
    "autoscale": {
        "min_workers": 2,
        "max_workers": 8
    },
    "spark_conf": {
        "spark.databricks.delta.preview.enabled": "true"
    },
    "auto_termination_minutes": 120  # Auto-terminate after 2 hours of inactivity
}

# Job Cluster (Automated workloads - more cost-effective)
job_cluster_config = {
    "new_cluster": {
        "spark_version": "13.3.x-scala2.12",
        "node_type_id": "Standard_DS3_v2",
        "num_workers": 4,
        "spark_conf": {
            "spark.databricks.delta.optimizeWrite.enabled": "true"
        }
    }
}
```


### ğŸ“¦ Delta Lake Basics

**Delta Lake** is an open-source storage layer that brings ACID transactions to data lakes. Think of it as adding **database superpowers** âš¡ to your data lake!

**Key Features:**

- ACID transactions ğŸ”’
- Time travel (version history) â°
- Schema enforcement and evolution ğŸ“‹
- Unified batch and streaming ğŸŒŠ

```python
# Creating a Delta table
from delta.tables import DeltaTable

# Write DataFrame as Delta table
(df
    .write
    .format("delta")
    .mode("overwrite")
    .option("overwriteSchema", "true")
    .save("/mnt/datalake/processed/customers/")
)

# Register as SQL table
spark.sql("""
    CREATE TABLE IF NOT EXISTS customers
    USING DELTA
    LOCATION '/mnt/datalake/processed/customers/'
""")

# ACID transactions - MERGE operation (Upsert)
delta_table = DeltaTable.forPath(spark, "/mnt/datalake/processed/customers/")

# Merge new data
(delta_table.alias("target")
    .merge(
        new_data.alias("source"),
        "target.customer_id = source.customer_id"
    )
    .whenMatchedUpdateAll()  # Update existing records
    .whenNotMatchedInsertAll()  # Insert new records
    .execute()
)

# Time Travel - query previous versions
df_version_5 = spark.read.format("delta").option("versionAsOf", 5).load("/mnt/path/")
df_yesterday = spark.read.format("delta").option("timestampAsOf", "2025-01-09").load("/mnt/path/")

# View history
delta_table.history().show()

# Optimize and vacuum
delta_table.optimize().executeCompaction()
delta_table.vacuum(168)  # Remove files older than 7 days (168 hours)
```

**Schema Evolution:**

```python
# Enable schema evolution
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")

# Add new columns automatically
new_df_with_extra_cols = old_df.withColumn("new_column", lit("default_value"))

(new_df_with_extra_cols
    .write
    .format("delta")
    .mode("append")
    .option("mergeSchema", "true")
    .save("/mnt/datalake/processed/customers/")
)
```


### ğŸ›¡ï¸ Unity Catalog for Data Governance

**Unity Catalog** is Databricks' unified governance solution for data and AI. It's like a **central control tower** ğŸ° for all your data assets!

**Three-Level Namespace:**

- **Catalog** â†’ Database container
- **Schema (Database)** â†’ Table container
- **Table/View** â†’ Actual data

```python
# Creating catalog structure
spark.sql("CREATE CATALOG IF NOT EXISTS production")
spark.sql("USE CATALOG production")

spark.sql("CREATE SCHEMA IF NOT EXISTS sales_data")
spark.sql("USE SCHEMA sales_data")

# Create managed table in Unity Catalog
spark.sql("""
    CREATE TABLE IF NOT EXISTS customers (
        customer_id BIGINT,
        name STRING,
        email STRING,
        country STRING,
        created_date DATE
    )
    USING DELTA
    PARTITIONED BY (country)
""")

# Grant permissions
spark.sql("""
    GRANT SELECT ON TABLE production.sales_data.customers 
    TO `analysts@company.com`
""")

spark.sql("""
    GRANT MODIFY ON TABLE production.sales_data.customers 
    TO `data_engineers@company.com`
""")

# Create view with row-level security
spark.sql("""
    CREATE VIEW production.sales_data.customers_secured AS
    SELECT * FROM production.sales_data.customers
    WHERE country = current_user()  -- Filter based on user
""")

# Audit and lineage
spark.sql("""
    SELECT * FROM system.access.audit
    WHERE table_name = 'customers'
    ORDER BY event_time DESC
""")
```

**Data Discovery:**

```python
# Search for tables
spark.sql("SHOW TABLES IN production.sales_data").show()

# Describe table details
spark.sql("DESCRIBE EXTENDED production.sales_data.customers").show()

# View table properties
spark.sql("SHOW TBLPROPERTIES production.sales_data.customers").show()
```


***

## ğŸ¯ Mini Project: End-to-End Azure Data Pipeline

Let's build a complete data pipeline integrating all concepts! ğŸš€

### Project Architecture:

1. ğŸ“¥ Ingest data from ADLS Gen2 using ADF
2. ğŸ”„ Transform using PySpark in Databricks
3. ğŸ’¾ Load into Delta Lake
4. ğŸ“Š Create curated views for analytics

### Step 1: Azure Data Factory Pipeline

```json
{
  "name": "CustomerDataPipeline",
  "properties": {
    "activities": [
      {
        "name": "CopyToRawZone",
        "type": "Copy",
        "inputs": [
          {
            "referenceName": "SourceCSV",
            "type": "DatasetReference"
          }
        ],
        "outputs": [
          {
            "referenceName": "RawADLS",
            "type": "DatasetReference"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "DelimitedTextSource"
          },
          "sink": {
            "type": "DelimitedTextSink"
          }
        }
      },
      {
        "name": "TriggerDatabricksNotebook",
        "type": "DatabricksNotebook",
        "dependsOn": [
          {
            "activity": "CopyToRawZone",
            "dependencyConditions": ["Succeeded"]
          }
        ],
        "typeProperties": {
          "notebookPath": "/Workspace/ETL/ProcessCustomerData",
          "baseParameters": {
            "input_path": "/mnt/datalake/raw/customers/",
            "output_path": "/mnt/datalake/processed/customers/"
          }
        },
        "linkedServiceName": {
          "referenceName": "DatabricksLinkedService",
          "type": "LinkedServiceReference"
        }
      }
    ]
  }
}
```


### Step 2: Databricks Transformation Notebook

```python
# Notebook: ProcessCustomerData
# ==================================

# Widget for parameters
dbutils.widgets.text("input_path", "/mnt/datalake/raw/customers/")
dbutils.widgets.text("output_path", "/mnt/datalake/processed/customers/")

input_path = dbutils.widgets.get("input_path")
output_path = dbutils.widgets.get("output_path")

# Imports
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta.tables import *

print(f"ğŸ“¥ Reading data from: {input_path}")

# Read raw data
raw_df = (spark.read
    .format("csv")
    .option("header", "true")
    .option("inferSchema", "true")
    .load(input_path)
)

print(f"ğŸ“Š Raw record count: {raw_df.count()}")

# Data Quality Checks
print("ğŸ” Performing data quality checks...")

# Check for nulls in critical columns
null_checks = raw_df.select([
    sum(col(c).isNull().cast("int")).alias(c) 
    for c in ["customer_id", "email"]
])
display(null_checks)

# Transformations
print("ğŸ”„ Applying transformations...")

cleaned_df = (raw_df
    # Remove duplicates
    .dropDuplicates(["customer_id"])
    
    # Filter out invalid records
    .filter(col("customer_id").isNotNull())
    .filter(col("email").isNotNull())
    
    # Standardize email
    .withColumn("email", lower(trim(col("email"))))
    
    # Clean phone numbers
    .withColumn("phone", regexp_replace(col("phone"), "[^0-9]", ""))
    
    # Parse dates
    .withColumn("created_date", to_date(col("created_date"), "yyyy-MM-dd"))
    
    # Add metadata
    .withColumn("processed_timestamp", current_timestamp())
    .withColumn("processing_date", current_date())
    
    # Derive new columns
    .withColumn("customer_age_days", 
                datediff(current_date(), col("created_date")))
    .withColumn("customer_segment",
                when(col("customer_age_days") < 30, "New")
                .when(col("customer_age_days") < 365, "Regular")
                .otherwise("Loyal"))
)

print(f"âœ… Cleaned record count: {cleaned_df.count()}")

# Write to Silver layer (Delta)
print(f"ğŸ’¾ Writing to: {output_path}")

(cleaned_df
    .write
    .format("delta")
    .mode("overwrite")
    .partitionBy("processing_date", "customer_segment")
    .option("overwriteSchema", "true")
    .save(output_path)
)

# Create/Update Delta table
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS customers_silver
    USING DELTA
    LOCATION '{output_path}'
""")

# Optimize table
spark.sql("OPTIMIZE customers_silver")

# Collect statistics
spark.sql("ANALYZE TABLE customers_silver COMPUTE STATISTICS")

print("âœ¨ Processing complete!")

# Display sample
display(spark.table("customers_silver").limit(10))
```


### Step 3: Create Gold Layer Analytics

```python
# Notebook: CreateCustomer360View
# ==================================

from pyspark.sql.functions import *

# Read Silver tables
customers = spark.table("customers_silver")

# Assume we have orders table
orders = spark.read.format("delta").load("/mnt/datalake/processed/orders/")

# Create Customer 360 view
customer_360 = (customers
    .join(
        orders.groupBy("customer_id").agg(
            count("*").alias("total_orders"),
            sum("order_amount").alias("total_spent"),
            avg("order_amount").alias("avg_order_value"),
            max("order_date").alias("last_order_date"),
            min("order_date").alias("first_order_date")
        ),
        "customer_id",
        "left"
    )
    .withColumn("days_since_last_order",
                datediff(current_date(), col("last_order_date")))
    .withColumn("customer_lifetime_days",
                datediff(current_date(), col("first_order_date")))
    .withColumn("purchase_frequency",
                col("total_orders") / col("customer_lifetime_days"))
)

# Write to Gold layer
gold_path = "/mnt/datalake/curated/customer_360/"

(customer_360
    .write
    .format("delta")
    .mode("overwrite")
    .save(gold_path)
)

# Register table in Unity Catalog
spark.sql(f"""
    CREATE OR REPLACE TABLE production.analytics.customer_360
    USING DELTA
    LOCATION '{gold_path}'
""")

# Grant access to analysts
spark.sql("""
    GRANT SELECT ON TABLE production.analytics.customer_360 
    TO `analysts@company.com`
""")

print("ğŸ‰ Customer 360 view created successfully!")
display(spark.table("production.analytics.customer_360").limit(20))
```


### Step 4: Monitoring and Alerts

```python
# Add monitoring to ADF pipeline
{
  "name": "CheckDataQuality",
  "type": "IfCondition",
  "dependsOn": [
    {
      "activity": "TriggerDatabricksNotebook",
      "dependencyConditions": ["Succeeded"]
    }
  ],
  "typeProperties": {
    "expression": {
      "@greater(activity('TriggerDatabricksNotebook').output.recordCount, 0)"
    },
    "ifFalseActivities": [
      {
        "name": "SendFailureEmail",
        "type": "WebActivity",
        "typeProperties": {
          "url": "https://logic-app-url/trigger",
          "method": "POST",
          "body": {
            "message": "Pipeline failed: No records processed"
          }
        }
      }
    ]
  }
}
```


***

## ğŸ“ Key Takeaways

- **Azure Data Factory** orchestrates data movement and transformation pipelines âš™ï¸
- **Integration Runtimes** provide the compute infrastructure for data integration ğŸ’»
- **ADLS Gen2** offers scalable, hierarchical storage for big data ğŸ—„ï¸
- **Medallion Architecture** (Bronze/Silver/Gold) organizes data by refinement level ğŸ…
- **Delta Lake** adds ACID transactions and time travel to data lakes ğŸ“¦
- **Unity Catalog** provides centralized governance and security ğŸ›¡ï¸
- **End-to-end pipelines** integrate all components for production workflows ğŸš€

What aspect would you like to dive deeper into? Would you prefer more advanced Delta Lake features, Unity Catalog governance patterns, or ADF performance optimization techniques? ğŸ¤”

